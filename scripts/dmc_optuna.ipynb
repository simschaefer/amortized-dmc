{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../BayesFlow\")\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import os\n",
    "if \"KERAS_BACKEND\" not in os.environ:\n",
    "    # set this to \"torch\", \"tensorflow\", or \"jax\"\n",
    "    os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import keras\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function Trials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bayesflow as bf\n",
    "from dmc import DMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator = DMC(\n",
    "    prior_means=np.array([16., 111., 0.5, 322., 75.]), \n",
    "    prior_sds=np.array([10., 47., 0.13, 40., 23.]),\n",
    "    tmax=1500,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter = (\n",
    "    bf.adapters.Adapter()\n",
    "    .convert_dtype(\"float64\", \"float32\")\n",
    "    .sqrt(\"num_obs\")\n",
    "    .concatenate([\"A\", \"tau\", \"mu_c\", \"t0\", \"b\"], into=\"inference_variables\")\n",
    "    .concatenate([\"rt\", \"accuracy\", \"conditions\"], into=\"summary_variables\")\n",
    "    .standardize(include=\"inference_variables\")\n",
    "    .rename(\"num_obs\", \"inference_conditions\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file_path = '../data/data_offline_training/data_offline_training.pickle'\n",
    "\n",
    "with open(training_file_path, 'rb') as file:\n",
    "    train_data = pickle.load(file)\n",
    "\n",
    "    \n",
    "val_file_path = '../data/data_offline_training/data_offline_validation.pickle'\n",
    "\n",
    "with open(val_file_path, 'rb') as file:\n",
    "    val_data = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_metric_sum(metrics_table, weight_recovery=1, weight_pc=1, weight_sbc=1):\n",
    "    \n",
    "    # recode posterior contraction\n",
    "    metrics_table.iloc[1,:]=1-metrics_table.iloc[1,:]\n",
    "\n",
    "    # compute means across parameters\n",
    "    metrics_means=metrics_table.mean(axis=1)\n",
    "\n",
    "    # decide on weights for each metric (Recovery, Posterior Contraction, SBC)\n",
    "    metrics_weights=np.array([weight_recovery, weight_pc, weight_sbc])\n",
    "\n",
    "    # compute weighted sum\n",
    "    weighted_sum=np.dot(metrics_means, metrics_weights)\n",
    "    \n",
    "    return weighted_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bayesflow:Fitting on dataset instance of OfflineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 83/391\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 66ms/step - loss: 6.4618 - loss/inference_loss: 6.4618"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 43\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# loss=np.mean(history.history[\"val_loss\"][-5:])\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m weighted_sum\n\u001b[0;32m---> 43\u001b[0m objective_test\u001b[38;5;241m=\u001b[39mobjective()\n",
      "Cell \u001b[0;32mIn[64], line 32\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(epochs)\u001b[0m\n\u001b[1;32m     19\u001b[0m summary_net \u001b[38;5;241m=\u001b[39m bf\u001b[38;5;241m.\u001b[39mnetworks\u001b[38;5;241m.\u001b[39mSetTransformer(summary_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, num_seeds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m     22\u001b[0m workflow \u001b[38;5;241m=\u001b[39m bf\u001b[38;5;241m.\u001b[39mBasicWorkflow(\n\u001b[1;32m     23\u001b[0m     simulator\u001b[38;5;241m=\u001b[39msimulator,\n\u001b[1;32m     24\u001b[0m     adapter\u001b[38;5;241m=\u001b[39madapter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# checkpoint_name= \"simons_crazy_net3\",\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     inference_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtau\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmu_c\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt0\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 32\u001b[0m history \u001b[38;5;241m=\u001b[39m workflow\u001b[38;5;241m.\u001b[39mfit_offline(train_data, epochs\u001b[38;5;241m=\u001b[39mepochs, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, validation_data\u001b[38;5;241m=\u001b[39mval_data)\n\u001b[1;32m     34\u001b[0m metrics_table\u001b[38;5;241m=\u001b[39mworkflow\u001b[38;5;241m.\u001b[39mcompute_default_diagnostics(test_data\u001b[38;5;241m=\u001b[39mval_data)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# compute weighted sum\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/bayesflow/workflows/basic_workflow.py:668\u001b[0m, in \u001b[0;36mBasicWorkflow.fit_offline\u001b[0;34m(self, data, epochs, batch_size, keep_optimizer, validation_data, **kwargs)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;124;03mTrain the approximator offline using a fixed dataset. This approach will be faster than online training,\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;124;03msince no computation time is spent in generating new data for each batch, but it assumes that simulations\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;124;03m    metric evolution over epochs.\u001b[39;00m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    666\u001b[0m dataset \u001b[38;5;241m=\u001b[39m OfflineDataset(data\u001b[38;5;241m=\u001b[39mdata, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, adapter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madapter)\n\u001b[0;32m--> 668\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[1;32m    669\u001b[0m     dataset, epochs, strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monline\u001b[39m\u001b[38;5;124m\"\u001b[39m, keep_optimizer\u001b[38;5;241m=\u001b[39mkeep_optimizer, validation_data\u001b[38;5;241m=\u001b[39mvalidation_data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    670\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/bayesflow/workflows/basic_workflow.py:858\u001b[0m, in \u001b[0;36mBasicWorkflow._fit\u001b[0;34m(self, dataset, epochs, strategy, keep_optimizer, validation_data, **kwargs)\u001b[0m\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapproximator\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, metrics\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetrics\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 858\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapproximator\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m    859\u001b[0m         dataset\u001b[38;5;241m=\u001b[39mdataset, epochs\u001b[38;5;241m=\u001b[39mepochs, validation_data\u001b[38;5;241m=\u001b[39mvalidation_data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    860\u001b[0m     )\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_training_finished()\n\u001b[1;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/bayesflow/approximators/continuous_approximator.py:202\u001b[0m, in \u001b[0;36mContinuousApproximator.fit\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    151\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;124;03m    Trains the approximator on the provided dataset or on-demand data generated from the given simulator.\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m    If `dataset` is not provided, a dataset is built from the `simulator`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03m        If both `dataset` and `simulator` are provided or neither is provided.\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, adapter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madapter)\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/bayesflow/approximators/approximator.py:136\u001b[0m, in \u001b[0;36mApproximator.fit\u001b[0;34m(self, dataset, simulator, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m     mock_data \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mtree\u001b[38;5;241m.\u001b[39mmap_structure(keras\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mconvert_to_tensor, mock_data)\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_from_data(mock_data)\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(dataset\u001b[38;5;241m=\u001b[39mdataset, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/bayesflow/approximators/backend_approximators/backend_approximator.py:22\u001b[0m, in \u001b[0;36mBackendApproximator.fit\u001b[0;34m(self, dataset, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, dataset: keras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mPyDataset, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(x\u001b[38;5;241m=\u001b[39mdataset, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfilter_kwargs(kwargs, \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit))\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/keras/src/backend/torch/trainer.py:257\u001b[0m, in \u001b[0;36mTorchTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, data \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;66;03m# Callbacks\u001b[39;00m\n\u001b[1;32m    255\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 257\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(data)\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Callbacks\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/keras/src/backend/torch/trainer.py:117\u001b[0m, in \u001b[0;36mTorchTrainer.make_train_function.<locals>.one_step_on_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs a single training step on a batch of data.\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m data \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_step(data)\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/bayesflow/approximators/backend_approximators/torch_approximator.py:33\u001b[0m, in \u001b[0;36mTorchApproximator.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mapply(gradients, trainable_weights)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss_tracker\u001b[38;5;241m.\u001b[39mupdate_state(loss)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m metrics\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py:448\u001b[0m, in \u001b[0;36mBaseOptimizer.apply\u001b[0;34m(self, grads, trainable_variables)\u001b[0m\n\u001b[1;32m    445\u001b[0m     grads \u001b[38;5;241m=\u001b[39m [g \u001b[38;5;28;01mif\u001b[39;00m g \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m g \u001b[38;5;241m/\u001b[39m scale \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m grads]\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# Apply gradient updates.\u001b[39;00m\n\u001b[0;32m--> 448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend_apply_gradients(grads, trainable_variables)\n\u001b[1;32m    449\u001b[0m \u001b[38;5;66;03m# Apply variable constraints after applying gradients.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m variable \u001b[38;5;129;01min\u001b[39;00m trainable_variables:\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py:507\u001b[0m, in \u001b[0;36mBaseOptimizer._backend_apply_gradients\u001b[0;34m(self, grads, trainable_variables)\u001b[0m\n\u001b[1;32m    498\u001b[0m     ops\u001b[38;5;241m.\u001b[39mcond(\n\u001b[1;32m    499\u001b[0m         is_update_step,\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: _update_step_fn(grads, trainable_variables),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    503\u001b[0m         ),\n\u001b[1;32m    504\u001b[0m     )\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;66;03m# Apply clipping and weight decay.\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m     grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clip_gradients(grads)\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_weight_decay(trainable_variables)\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;66;03m# Run update step.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py:784\u001b[0m, in \u001b[0;36mBaseOptimizer._clip_gradients\u001b[0;34m(self, grads)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_clip_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m, grads):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclipnorm \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclipnorm \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 784\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    785\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clip_by_norm(g) \u001b[38;5;28;01mif\u001b[39;00m g \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m g \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m grads\n\u001b[1;32m    786\u001b[0m         ]\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_clipnorm \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_clipnorm \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    788\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m clip_by_global_norm(grads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_clipnorm)\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py:785\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_clip_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m, grads):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclipnorm \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclipnorm \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 785\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clip_by_norm(g) \u001b[38;5;28;01mif\u001b[39;00m g \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m g \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m grads\n\u001b[1;32m    786\u001b[0m         ]\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_clipnorm \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_clipnorm \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    788\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m clip_by_global_norm(grads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_clipnorm)\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py:1018\u001b[0m, in \u001b[0;36mBaseOptimizer._clip_by_norm\u001b[0;34m(self, values, axes)\u001b[0m\n\u001b[1;32m   1016\u001b[0m pred \u001b[38;5;241m=\u001b[39m l2sum \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;66;03m# Two-tap tf.where trick to bypass NaN gradients\u001b[39;00m\n\u001b[0;32m-> 1018\u001b[0m l2sum_safe \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mwhere(pred, l2sum, ops\u001b[38;5;241m.\u001b[39mones_like(l2sum))\n\u001b[1;32m   1019\u001b[0m l2norm \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mwhere(pred, ops\u001b[38;5;241m.\u001b[39msqrt(l2sum_safe), l2sum)\n\u001b[1;32m   1020\u001b[0m intermediate \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mmultiply(values, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclipnorm)\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/keras/src/ops/numpy.py:4398\u001b[0m, in \u001b[0;36mones_like\u001b[0;34m(x, dtype)\u001b[0m\n\u001b[1;32m   4396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors((x,)):\n\u001b[1;32m   4397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m OnesLike()\u001b[38;5;241m.\u001b[39msymbolic_call(x, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m-> 4398\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mnumpy\u001b[38;5;241m.\u001b[39mones_like(x, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/keras/src/backend/torch/numpy.py:1121\u001b[0m, in \u001b[0;36mones_like\u001b[0;34m(x, dtype)\u001b[0m\n\u001b[1;32m   1119\u001b[0m x \u001b[38;5;241m=\u001b[39m convert_to_tensor(x)\n\u001b[1;32m   1120\u001b[0m dtype \u001b[38;5;241m=\u001b[39m to_torch_dtype(dtype \u001b[38;5;129;01mor\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m-> 1121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mones_like(x, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### define objective function\n",
    "\n",
    "def objective(epochs=1):\n",
    "\n",
    "    # Optimize hyperparameters\n",
    "    # dropout = trial.suggest_float(\"dropout\", 0.01, 0.5)\n",
    "    # initial_learning_rate = trial.suggest_float(\"lr\", 1e-4, 1e-3) \n",
    "    \n",
    "    dropout = 0.1\n",
    "    initial_learning_rate = 5e-4\n",
    "    batch_size=128\n",
    "    \n",
    "    # Create inference net\n",
    "    \n",
    "    inference_net = bf.networks.CouplingFlow(coupling_kwargs=dict(subnet_kwargs=dict(dropout=dropout)))\n",
    "\n",
    "    # inference_net = bf.networks.FlowMatching(subnet_kwargs=dict(dropout=0.1))\n",
    "\n",
    "    summary_net = bf.networks.SetTransformer(summary_dim=32, num_seeds=2, dropout=0.1)\n",
    "    \n",
    "    \n",
    "    workflow = bf.BasicWorkflow(\n",
    "        simulator=simulator,\n",
    "        adapter=adapter,\n",
    "        initial_learning_rate=initial_learning_rate,\n",
    "        inference_network=inference_net,\n",
    "        summary_network=summary_net,\n",
    "        # checkpoint_filepath='../checkpoints',\n",
    "        # checkpoint_name= \"simons_crazy_net3\",\n",
    "        inference_variables=[\"A\", \"tau\", \"mu_c\", \"t0\", \"b\"])\n",
    "    \n",
    "    history = workflow.fit_offline(train_data, epochs=epochs, batch_size=batch_size, validation_data=val_data)\n",
    "    \n",
    "    metrics_table=workflow.compute_default_diagnostics(test_data=val_data)\n",
    "\n",
    "    # compute weighted sum\n",
    "    weighted_sum=weighted_metric_sum(metrics_table)\n",
    "    \n",
    "    # loss=np.mean(history.history[\"val_loss\"][-5:])\n",
    "        \n",
    "    return weighted_sum\n",
    "\n",
    "objective_test=objective()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8598938856409903"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objective_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def objective(trial, epochs=50):\n",
    "\n",
    "    # Optimize hyperparameters\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.01, 0.3)\n",
    "    initial_learning_rate = trial.suggest_float(\"lr\", 1e-4, 1e-3) \n",
    "    num_seeds=trial.suggest_int(\"num_seeds\", 1, 4)\n",
    "    depth=trial.suggest_int(\"depth\", 5, 10)\n",
    "    \n",
    "    batch_size=128\n",
    "    \n",
    "    # Create inference net \n",
    "    inference_net = bf.networks.CouplingFlow(coupling_kwargs=dict(subnet_kwargs=dict(dropout=dropout)), depth=depth)\n",
    "\n",
    "    summary_net = bf.networks.SetTransformer(summary_dim=32, num_seeds=num_seeds, dropout=dropout)\n",
    "    \n",
    "    \n",
    "    workflow = bf.BasicWorkflow(\n",
    "        simulator=simulator,\n",
    "        adapter=adapter,\n",
    "        initial_learning_rate=initial_learning_rate,\n",
    "        inference_network=inference_net,\n",
    "        summary_network=summary_net,\n",
    "        # checkpoint_filepath='../checkpoints',\n",
    "        # checkpoint_name= \"simons_crazy_net3\",\n",
    "        inference_variables=[\"A\", \"tau\", \"mu_c\", \"t0\", \"b\"])\n",
    "    \n",
    "    history = workflow.fit_offline(train_data, epochs=epochs, batch_size=batch_size, validation_data=val_data, verbose=0)\n",
    "    \n",
    "    metrics_table=workflow.compute_default_diagnostics(test_data=val_data)\n",
    "\n",
    "    # compute weighted sum\n",
    "    weighted_sum=weighted_metric_sum(metrics_table)\n",
    "    \n",
    "    # loss=np.mean(history.history[\"val_loss\"][-5:])\n",
    "        \n",
    "    return weighted_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-02 18:57:01,588] A new study created in memory with name: no-name-3df42576-5ff5-4a51-863e-3aa99e7bac20\n",
      "INFO:bayesflow:Fitting on dataset instance of OfflineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n",
      "[I 2025-04-02 19:20:41,821] Trial 0 finished with value: 0.3671495569967654 and parameters: {'dropout': 0.14897769957851867, 'lr': 0.000869910397329933, 'num_seeds': 4, 'depth': 8}. Best is trial 0 with value: 0.3671495569967654.\n",
      "INFO:bayesflow:Fitting on dataset instance of OfflineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n",
      "[I 2025-04-02 19:39:47,068] Trial 1 finished with value: 0.4268942741742283 and parameters: {'dropout': 0.21885899188349522, 'lr': 0.0008756214916702613, 'num_seeds': 2, 'depth': 5}. Best is trial 0 with value: 0.3671495569967654.\n",
      "INFO:bayesflow:Fitting on dataset instance of OfflineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n",
      "[I 2025-04-02 20:04:12,895] Trial 2 finished with value: 0.3311017336289652 and parameters: {'dropout': 0.28479278384893564, 'lr': 0.0006289298441233264, 'num_seeds': 2, 'depth': 9}. Best is trial 2 with value: 0.3311017336289652.\n",
      "INFO:bayesflow:Fitting on dataset instance of OfflineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n",
      "[I 2025-04-02 20:25:38,524] Trial 3 finished with value: 0.2698363286229111 and parameters: {'dropout': 0.017054666664286084, 'lr': 0.000824992852113653, 'num_seeds': 2, 'depth': 7}. Best is trial 3 with value: 0.2698363286229111.\n",
      "INFO:bayesflow:Fitting on dataset instance of OfflineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n",
      "[I 2025-04-02 20:44:06,334] Trial 4 finished with value: 0.404253411542282 and parameters: {'dropout': 0.2897391239688206, 'lr': 0.0005483322454254692, 'num_seeds': 2, 'depth': 5}. Best is trial 3 with value: 0.2698363286229111.\n",
      "INFO:bayesflow:Fitting on dataset instance of OfflineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n",
      "[I 2025-04-02 21:09:55,147] Trial 5 finished with value: 0.4029528410076702 and parameters: {'dropout': 0.2875397118008504, 'lr': 0.00037843696723906326, 'num_seeds': 1, 'depth': 10}. Best is trial 3 with value: 0.2698363286229111.\n",
      "INFO:bayesflow:Fitting on dataset instance of OfflineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n",
      "[I 2025-04-02 21:32:48,841] Trial 6 finished with value: 0.2932635588998633 and parameters: {'dropout': 0.13006465644703658, 'lr': 0.00041305823859640327, 'num_seeds': 3, 'depth': 8}. Best is trial 3 with value: 0.2698363286229111.\n",
      "INFO:bayesflow:Fitting on dataset instance of OfflineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n",
      "[I 2025-04-02 21:55:37,305] Trial 7 finished with value: 0.37112822221392894 and parameters: {'dropout': 0.153810565274244, 'lr': 0.0006478672613176672, 'num_seeds': 1, 'depth': 8}. Best is trial 3 with value: 0.2698363286229111.\n",
      "INFO:bayesflow:Fitting on dataset instance of OfflineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n",
      "[I 2025-04-02 22:16:59,034] Trial 8 finished with value: 0.3975272355338929 and parameters: {'dropout': 0.1817317551381419, 'lr': 0.0003056721280450969, 'num_seeds': 3, 'depth': 7}. Best is trial 3 with value: 0.2698363286229111.\n",
      "INFO:bayesflow:Fitting on dataset instance of OfflineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n",
      "[I 2025-04-02 22:38:21,955] Trial 9 finished with value: 0.3803885779709398 and parameters: {'dropout': 0.18809055837962588, 'lr': 0.0008615745423360212, 'num_seeds': 3, 'depth': 7}. Best is trial 3 with value: 0.2698363286229111.\n",
      "INFO:bayesflow:Fitting on dataset instance of OfflineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n",
      "[I 2025-04-02 22:58:17,339] Trial 10 finished with value: 0.2773580881955304 and parameters: {'dropout': 0.02071759557746684, 'lr': 0.00015458189280985793, 'num_seeds': 4, 'depth': 6}. Best is trial 3 with value: 0.2698363286229111.\n",
      "INFO:bayesflow:Fitting on dataset instance of OfflineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n",
      "[I 2025-04-02 23:18:12,007] Trial 11 finished with value: 0.2803402647879205 and parameters: {'dropout': 0.012726439313986265, 'lr': 0.00013142478308393837, 'num_seeds': 4, 'depth': 6}. Best is trial 3 with value: 0.2698363286229111.\n",
      "INFO:bayesflow:Fitting on dataset instance of OfflineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n",
      "[I 2025-04-02 23:38:10,336] Trial 12 finished with value: 0.27211182811131235 and parameters: {'dropout': 0.018600077342498765, 'lr': 0.0009964779668079835, 'num_seeds': 4, 'depth': 6}. Best is trial 3 with value: 0.2698363286229111.\n",
      "INFO:bayesflow:Fitting on dataset instance of OfflineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n",
      "[I 2025-04-02 23:58:07,778] Trial 13 finished with value: 0.2895723469604218 and parameters: {'dropout': 0.07812981961879426, 'lr': 0.0009733764314027756, 'num_seeds': 2, 'depth': 6}. Best is trial 3 with value: 0.2698363286229111.\n",
      "INFO:bayesflow:Fitting on dataset instance of OfflineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n",
      "[I 2025-04-03 00:19:32,134] Trial 14 finished with value: 0.2956929478785452 and parameters: {'dropout': 0.05960046169649995, 'lr': 0.0009900959102802022, 'num_seeds': 3, 'depth': 7}. Best is trial 3 with value: 0.2698363286229111.\n",
      "INFO:bayesflow:Fitting on dataset instance of OfflineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n",
      "[I 2025-04-03 00:39:24,691] Trial 15 finished with value: 0.2854624727651972 and parameters: {'dropout': 0.08686984541891556, 'lr': 0.0007713521005039458, 'num_seeds': 1, 'depth': 6}. Best is trial 3 with value: 0.2698363286229111.\n",
      "INFO:bayesflow:Fitting on dataset instance of OfflineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n",
      "[I 2025-04-03 00:57:54,473] Trial 16 finished with value: 0.27685191565086603 and parameters: {'dropout': 0.045467421842948164, 'lr': 0.0007497713073778742, 'num_seeds': 4, 'depth': 5}. Best is trial 3 with value: 0.2698363286229111.\n",
      "INFO:bayesflow:Fitting on dataset instance of OfflineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n",
      "[I 2025-04-03 01:22:32,320] Trial 17 finished with value: 0.37373370723980864 and parameters: {'dropout': 0.09989623745574452, 'lr': 0.0007556645406616644, 'num_seeds': 3, 'depth': 9}. Best is trial 3 with value: 0.2698363286229111.\n",
      "INFO:bayesflow:Fitting on dataset instance of OfflineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n",
      "[I 2025-04-03 01:43:59,458] Trial 18 finished with value: 0.29398137284611964 and parameters: {'dropout': 0.049214138704848656, 'lr': 0.0009298839609601276, 'num_seeds': 2, 'depth': 7}. Best is trial 3 with value: 0.2698363286229111.\n",
      "INFO:bayesflow:Fitting on dataset instance of OfflineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n",
      "[I 2025-04-03 02:03:52,506] Trial 19 finished with value: 0.3590459412314487 and parameters: {'dropout': 0.11333341371917009, 'lr': 0.0006523481029034324, 'num_seeds': 1, 'depth': 6}. Best is trial 3 with value: 0.2698363286229111.\n",
      "INFO:bayesflow:Fitting on dataset instance of OfflineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n",
      "[I 2025-04-03 02:28:19,335] Trial 20 finished with value: 0.2765631271908677 and parameters: {'dropout': 0.029797146490984472, 'lr': 0.0005152456296795235, 'num_seeds': 4, 'depth': 9}. Best is trial 3 with value: 0.2698363286229111.\n",
      "INFO:bayesflow:Fitting on dataset instance of OfflineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n",
      "[I 2025-04-03 02:54:10,712] Trial 21 finished with value: 0.28023642580685265 and parameters: {'dropout': 0.02695573045884108, 'lr': 0.0005110786926653211, 'num_seeds': 4, 'depth': 10}. Best is trial 3 with value: 0.2698363286229111.\n",
      "INFO:bayesflow:Fitting on dataset instance of OfflineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n",
      "[I 2025-04-03 03:18:33,806] Trial 22 finished with value: 0.3001380666118588 and parameters: {'dropout': 0.06714502510200498, 'lr': 0.00047607007323982267, 'num_seeds': 4, 'depth': 9}. Best is trial 3 with value: 0.2698363286229111.\n",
      "INFO:bayesflow:Fitting on dataset instance of OfflineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n",
      "[I 2025-04-03 03:41:31,996] Trial 23 finished with value: 0.27039669191509863 and parameters: {'dropout': 0.010498828807018475, 'lr': 0.0008057687583770871, 'num_seeds': 3, 'depth': 8}. Best is trial 3 with value: 0.2698363286229111.\n",
      "INFO:bayesflow:Fitting on dataset instance of OfflineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n",
      "[I 2025-04-03 04:04:23,929] Trial 24 finished with value: 0.285492562207329 and parameters: {'dropout': 0.04319274149610572, 'lr': 0.0008153965471011827, 'num_seeds': 3, 'depth': 8}. Best is trial 3 with value: 0.2698363286229111.\n",
      "INFO:bayesflow:Fitting on dataset instance of OfflineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n",
      "[I 2025-04-03 04:25:47,561] Trial 25 finished with value: 0.2727625775673833 and parameters: {'dropout': 0.011567755110584083, 'lr': 0.0009211954684966778, 'num_seeds': 2, 'depth': 7}. Best is trial 3 with value: 0.2698363286229111.\n",
      "INFO:bayesflow:Fitting on dataset instance of OfflineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n",
      "[I 2025-04-03 04:48:41,242] Trial 26 finished with value: 0.4872093697556666 and parameters: {'dropout': 0.2508067035586844, 'lr': 0.0006924356338968826, 'num_seeds': 3, 'depth': 8}. Best is trial 3 with value: 0.2698363286229111.\n",
      "INFO:bayesflow:Fitting on dataset instance of OfflineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n",
      "[I 2025-04-03 05:10:05,407] Trial 27 finished with value: 0.29847210544932734 and parameters: {'dropout': 0.07247235906180508, 'lr': 0.000998907121458734, 'num_seeds': 2, 'depth': 7}. Best is trial 3 with value: 0.2698363286229111.\n",
      "INFO:bayesflow:Fitting on dataset instance of OfflineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n",
      "[I 2025-04-03 05:30:04,051] Trial 28 finished with value: 0.29135271923845757 and parameters: {'dropout': 0.037395228003485534, 'lr': 0.0008233600891260566, 'num_seeds': 3, 'depth': 6}. Best is trial 3 with value: 0.2698363286229111.\n",
      "INFO:bayesflow:Fitting on dataset instance of OfflineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n",
      "[I 2025-04-03 05:52:56,301] Trial 29 finished with value: 0.31490748793824586 and parameters: {'dropout': 0.12426776634975081, 'lr': 0.0008963197670275647, 'num_seeds': 4, 'depth': 8}. Best is trial 3 with value: 0.2698363286229111.\n",
      "INFO:bayesflow:Fitting on dataset instance of OfflineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n",
      "[I 2025-04-03 06:11:28,108] Trial 30 finished with value: 0.35229198960352803 and parameters: {'dropout': 0.10219749387503427, 'lr': 0.0007178781445785598, 'num_seeds': 3, 'depth': 5}. Best is trial 3 with value: 0.2698363286229111.\n",
      "INFO:bayesflow:Fitting on dataset instance of OfflineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n",
      "[I 2025-04-03 06:32:57,456] Trial 31 finished with value: 0.27904812294435494 and parameters: {'dropout': 0.010831515670749975, 'lr': 0.0009318139363009934, 'num_seeds': 2, 'depth': 7}. Best is trial 3 with value: 0.2698363286229111.\n",
      "INFO:bayesflow:Fitting on dataset instance of OfflineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n",
      "[I 2025-04-03 06:54:26,600] Trial 32 finished with value: 0.26759983484342403 and parameters: {'dropout': 0.011529281588535391, 'lr': 0.0008293769610382236, 'num_seeds': 2, 'depth': 7}. Best is trial 32 with value: 0.26759983484342403.\n",
      "INFO:bayesflow:Fitting on dataset instance of OfflineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n",
      "[I 2025-04-03 07:17:22,935] Trial 33 finished with value: 0.27968070372368736 and parameters: {'dropout': 0.05428869964116452, 'lr': 0.0008412663781054064, 'num_seeds': 2, 'depth': 8}. Best is trial 32 with value: 0.26759983484342403.\n",
      "INFO:bayesflow:Fitting on dataset instance of OfflineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n",
      "[I 2025-04-03 07:38:50,516] Trial 34 finished with value: 0.27926717790935124 and parameters: {'dropout': 0.02798689624557639, 'lr': 0.0006090338374555466, 'num_seeds': 2, 'depth': 7}. Best is trial 32 with value: 0.26759983484342403.\n",
      "INFO:bayesflow:Fitting on dataset instance of OfflineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n",
      "[I 2025-04-03 07:58:50,019] Trial 35 finished with value: 0.26980194221052956 and parameters: {'dropout': 0.03412681649406579, 'lr': 0.0007942976610771257, 'num_seeds': 2, 'depth': 6}. Best is trial 32 with value: 0.26759983484342403.\n",
      "INFO:bayesflow:Fitting on dataset instance of OfflineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n",
      "[I 2025-04-03 08:17:22,919] Trial 36 finished with value: 0.2746198773343748 and parameters: {'dropout': 0.03972606188045223, 'lr': 0.0007947366579220129, 'num_seeds': 2, 'depth': 5}. Best is trial 32 with value: 0.26759983484342403.\n",
      "INFO:bayesflow:Fitting on dataset instance of OfflineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n",
      "[I 2025-04-03 08:40:14,715] Trial 37 finished with value: 0.289570667331411 and parameters: {'dropout': 0.06272877728190607, 'lr': 0.000693453898623779, 'num_seeds': 1, 'depth': 8}. Best is trial 32 with value: 0.26759983484342403.\n",
      "INFO:bayesflow:Fitting on dataset instance of OfflineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n",
      "[I 2025-04-03 09:04:40,190] Trial 38 finished with value: 0.33176664428794495 and parameters: {'dropout': 0.1428779914329208, 'lr': 0.000869773125623435, 'num_seeds': 2, 'depth': 9}. Best is trial 32 with value: 0.26759983484342403.\n",
      "INFO:bayesflow:Fitting on dataset instance of OfflineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n",
      "[I 2025-04-03 09:26:03,691] Trial 39 finished with value: 0.29639946444341336 and parameters: {'dropout': 0.08962855047364679, 'lr': 0.0005994219288566619, 'num_seeds': 1, 'depth': 7}. Best is trial 32 with value: 0.26759983484342403.\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "\n",
    "study.optimize(objective, n_trials=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.26759983484342403\n",
      "Best hyperparameters: {'dropout': 0.011529281588535391, 'lr': 0.0008293769610382236, 'num_seeds': 2, 'depth': 7}\n"
     ]
    }
   ],
   "source": [
    "trial = study.best_trial\n",
    "print(\"Validation loss: {}\".format(trial.value))\n",
    "print(\"Best hyperparameters: {}\".format(trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dropout': 0.011529281588535391,\n",
       " 'lr': 0.0008293769610382236,\n",
       " 'num_seeds': 2,\n",
       " 'depth': 7}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial.params[\"dropout\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_net = bf.networks.CouplingFlow(coupling_kwargs=dict(subnet_kwargs=dict(dropout=trial.params[\"dropout\"])), depth=trial.params[\"depth\"])\n",
    "\n",
    "# inference_net = bf.networks.FlowMatching(subnet_kwargs=dict(dropout=0.1))\n",
    "\n",
    "summary_net = bf.networks.SetTransformer(summary_dim=32, num_seeds=trial.params[\"num_seeds\"], dropout=trial.params[\"dropout\"])\n",
    "\n",
    "workflow = bf.BasicWorkflow(\n",
    "    simulator=simulator,\n",
    "    adapter=adapter,\n",
    "    initial_learning_rate=trial.params[\"lr\"],\n",
    "    inference_network=inference_net,\n",
    "    summary_network=summary_net,\n",
    "    checkpoint_filepath='../checkpoints',\n",
    "    checkpoint_name= \"optuna_run1\",\n",
    "    inference_variables=[\"A\", \"tau\", \"mu_c\", \"t0\", \"b\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "\n",
    "history = workflow.fit_offline(train_data, epochs=50, batch_size=batch_size, validation_data=val_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
