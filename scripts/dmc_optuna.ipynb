{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../BayesFlow\")\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import os\n",
    "if \"KERAS_BACKEND\" not in os.environ:\n",
    "    # set this to \"torch\", \"tensorflow\", or \"jax\"\n",
    "    os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import keras\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function Trials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bayesflow as bf\n",
    "from dmc import DMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator = DMC(\n",
    "    prior_means=np.array([16., 111., 0.5, 322., 75.]), \n",
    "    prior_sds=np.array([10., 47., 0.13, 40., 23.]),\n",
    "    tmax=1500,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter = (\n",
    "    bf.adapters.Adapter()\n",
    "    .convert_dtype(\"float64\", \"float32\")\n",
    "    .sqrt(\"num_obs\")\n",
    "    .concatenate([\"A\", \"tau\", \"mu_c\", \"t0\", \"b\"], into=\"inference_variables\")\n",
    "    .concatenate([\"rt\", \"accuracy\", \"conditions\"], into=\"summary_variables\")\n",
    "    .standardize(include=\"inference_variables\")\n",
    "    .rename(\"num_obs\", \"inference_conditions\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file_path = '../data/data_offline_training/data_offline_training.pickle'\n",
    "\n",
    "with open(training_file_path, 'rb') as file:\n",
    "    train_data = pickle.load(file)\n",
    "\n",
    "    \n",
    "val_file_path = '../data/data_offline_training/data_offline_validation.pickle'\n",
    "\n",
    "with open(val_file_path, 'rb') as file:\n",
    "    val_data = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_metric_sum(metrics_table, weight_recovery=1, weight_pc=1, weight_sbc=1):\n",
    "    \n",
    "    # recode posterior contraction\n",
    "    metrics_table.iloc[1,:]=1-metrics_table.iloc[1,:]\n",
    "\n",
    "    # compute means across parameters\n",
    "    metrics_means=metrics_table.mean(axis=1)\n",
    "\n",
    "    # decide on weights for each metric (Recovery, Posterior Contraction, SBC)\n",
    "    metrics_weights=np.array([weight_recovery, weight_pc, weight_sbc])\n",
    "\n",
    "    # compute weighted sum\n",
    "    weighted_sum=np.dot(metrics_means, metrics_weights)\n",
    "    \n",
    "    return weighted_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bayesflow:Fitting on dataset instance of OfflineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 83/391\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 66ms/step - loss: 6.4618 - loss/inference_loss: 6.4618"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 43\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# loss=np.mean(history.history[\"val_loss\"][-5:])\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m weighted_sum\n\u001b[0;32m---> 43\u001b[0m objective_test\u001b[38;5;241m=\u001b[39mobjective()\n",
      "Cell \u001b[0;32mIn[64], line 32\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(epochs)\u001b[0m\n\u001b[1;32m     19\u001b[0m summary_net \u001b[38;5;241m=\u001b[39m bf\u001b[38;5;241m.\u001b[39mnetworks\u001b[38;5;241m.\u001b[39mSetTransformer(summary_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, num_seeds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m     22\u001b[0m workflow \u001b[38;5;241m=\u001b[39m bf\u001b[38;5;241m.\u001b[39mBasicWorkflow(\n\u001b[1;32m     23\u001b[0m     simulator\u001b[38;5;241m=\u001b[39msimulator,\n\u001b[1;32m     24\u001b[0m     adapter\u001b[38;5;241m=\u001b[39madapter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# checkpoint_name= \"simons_crazy_net3\",\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     inference_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtau\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmu_c\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt0\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 32\u001b[0m history \u001b[38;5;241m=\u001b[39m workflow\u001b[38;5;241m.\u001b[39mfit_offline(train_data, epochs\u001b[38;5;241m=\u001b[39mepochs, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, validation_data\u001b[38;5;241m=\u001b[39mval_data)\n\u001b[1;32m     34\u001b[0m metrics_table\u001b[38;5;241m=\u001b[39mworkflow\u001b[38;5;241m.\u001b[39mcompute_default_diagnostics(test_data\u001b[38;5;241m=\u001b[39mval_data)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# compute weighted sum\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/bayesflow/workflows/basic_workflow.py:668\u001b[0m, in \u001b[0;36mBasicWorkflow.fit_offline\u001b[0;34m(self, data, epochs, batch_size, keep_optimizer, validation_data, **kwargs)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;124;03mTrain the approximator offline using a fixed dataset. This approach will be faster than online training,\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;124;03msince no computation time is spent in generating new data for each batch, but it assumes that simulations\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;124;03m    metric evolution over epochs.\u001b[39;00m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    666\u001b[0m dataset \u001b[38;5;241m=\u001b[39m OfflineDataset(data\u001b[38;5;241m=\u001b[39mdata, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, adapter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madapter)\n\u001b[0;32m--> 668\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[1;32m    669\u001b[0m     dataset, epochs, strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monline\u001b[39m\u001b[38;5;124m\"\u001b[39m, keep_optimizer\u001b[38;5;241m=\u001b[39mkeep_optimizer, validation_data\u001b[38;5;241m=\u001b[39mvalidation_data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    670\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/bayesflow/workflows/basic_workflow.py:858\u001b[0m, in \u001b[0;36mBasicWorkflow._fit\u001b[0;34m(self, dataset, epochs, strategy, keep_optimizer, validation_data, **kwargs)\u001b[0m\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapproximator\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, metrics\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetrics\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 858\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapproximator\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m    859\u001b[0m         dataset\u001b[38;5;241m=\u001b[39mdataset, epochs\u001b[38;5;241m=\u001b[39mepochs, validation_data\u001b[38;5;241m=\u001b[39mvalidation_data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    860\u001b[0m     )\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_training_finished()\n\u001b[1;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/bayesflow/approximators/continuous_approximator.py:202\u001b[0m, in \u001b[0;36mContinuousApproximator.fit\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    151\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;124;03m    Trains the approximator on the provided dataset or on-demand data generated from the given simulator.\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m    If `dataset` is not provided, a dataset is built from the `simulator`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03m        If both `dataset` and `simulator` are provided or neither is provided.\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, adapter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madapter)\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/bayesflow/approximators/approximator.py:136\u001b[0m, in \u001b[0;36mApproximator.fit\u001b[0;34m(self, dataset, simulator, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m     mock_data \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mtree\u001b[38;5;241m.\u001b[39mmap_structure(keras\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mconvert_to_tensor, mock_data)\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_from_data(mock_data)\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(dataset\u001b[38;5;241m=\u001b[39mdataset, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/bayesflow/approximators/backend_approximators/backend_approximator.py:22\u001b[0m, in \u001b[0;36mBackendApproximator.fit\u001b[0;34m(self, dataset, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, dataset: keras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mPyDataset, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(x\u001b[38;5;241m=\u001b[39mdataset, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfilter_kwargs(kwargs, \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit))\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/keras/src/backend/torch/trainer.py:257\u001b[0m, in \u001b[0;36mTorchTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, data \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;66;03m# Callbacks\u001b[39;00m\n\u001b[1;32m    255\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 257\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(data)\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Callbacks\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/keras/src/backend/torch/trainer.py:117\u001b[0m, in \u001b[0;36mTorchTrainer.make_train_function.<locals>.one_step_on_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs a single training step on a batch of data.\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m data \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_step(data)\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/bayesflow/approximators/backend_approximators/torch_approximator.py:33\u001b[0m, in \u001b[0;36mTorchApproximator.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mapply(gradients, trainable_weights)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss_tracker\u001b[38;5;241m.\u001b[39mupdate_state(loss)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m metrics\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py:448\u001b[0m, in \u001b[0;36mBaseOptimizer.apply\u001b[0;34m(self, grads, trainable_variables)\u001b[0m\n\u001b[1;32m    445\u001b[0m     grads \u001b[38;5;241m=\u001b[39m [g \u001b[38;5;28;01mif\u001b[39;00m g \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m g \u001b[38;5;241m/\u001b[39m scale \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m grads]\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# Apply gradient updates.\u001b[39;00m\n\u001b[0;32m--> 448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend_apply_gradients(grads, trainable_variables)\n\u001b[1;32m    449\u001b[0m \u001b[38;5;66;03m# Apply variable constraints after applying gradients.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m variable \u001b[38;5;129;01min\u001b[39;00m trainable_variables:\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py:507\u001b[0m, in \u001b[0;36mBaseOptimizer._backend_apply_gradients\u001b[0;34m(self, grads, trainable_variables)\u001b[0m\n\u001b[1;32m    498\u001b[0m     ops\u001b[38;5;241m.\u001b[39mcond(\n\u001b[1;32m    499\u001b[0m         is_update_step,\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: _update_step_fn(grads, trainable_variables),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    503\u001b[0m         ),\n\u001b[1;32m    504\u001b[0m     )\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;66;03m# Apply clipping and weight decay.\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m     grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clip_gradients(grads)\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_weight_decay(trainable_variables)\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;66;03m# Run update step.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py:784\u001b[0m, in \u001b[0;36mBaseOptimizer._clip_gradients\u001b[0;34m(self, grads)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_clip_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m, grads):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclipnorm \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclipnorm \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 784\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    785\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clip_by_norm(g) \u001b[38;5;28;01mif\u001b[39;00m g \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m g \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m grads\n\u001b[1;32m    786\u001b[0m         ]\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_clipnorm \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_clipnorm \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    788\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m clip_by_global_norm(grads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_clipnorm)\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py:785\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_clip_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m, grads):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclipnorm \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclipnorm \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 785\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clip_by_norm(g) \u001b[38;5;28;01mif\u001b[39;00m g \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m g \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m grads\n\u001b[1;32m    786\u001b[0m         ]\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_clipnorm \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_clipnorm \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    788\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m clip_by_global_norm(grads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_clipnorm)\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py:1018\u001b[0m, in \u001b[0;36mBaseOptimizer._clip_by_norm\u001b[0;34m(self, values, axes)\u001b[0m\n\u001b[1;32m   1016\u001b[0m pred \u001b[38;5;241m=\u001b[39m l2sum \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;66;03m# Two-tap tf.where trick to bypass NaN gradients\u001b[39;00m\n\u001b[0;32m-> 1018\u001b[0m l2sum_safe \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mwhere(pred, l2sum, ops\u001b[38;5;241m.\u001b[39mones_like(l2sum))\n\u001b[1;32m   1019\u001b[0m l2norm \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mwhere(pred, ops\u001b[38;5;241m.\u001b[39msqrt(l2sum_safe), l2sum)\n\u001b[1;32m   1020\u001b[0m intermediate \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mmultiply(values, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclipnorm)\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/keras/src/ops/numpy.py:4398\u001b[0m, in \u001b[0;36mones_like\u001b[0;34m(x, dtype)\u001b[0m\n\u001b[1;32m   4396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors((x,)):\n\u001b[1;32m   4397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m OnesLike()\u001b[38;5;241m.\u001b[39msymbolic_call(x, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m-> 4398\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mnumpy\u001b[38;5;241m.\u001b[39mones_like(x, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/keras/src/backend/torch/numpy.py:1121\u001b[0m, in \u001b[0;36mones_like\u001b[0;34m(x, dtype)\u001b[0m\n\u001b[1;32m   1119\u001b[0m x \u001b[38;5;241m=\u001b[39m convert_to_tensor(x)\n\u001b[1;32m   1120\u001b[0m dtype \u001b[38;5;241m=\u001b[39m to_torch_dtype(dtype \u001b[38;5;129;01mor\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m-> 1121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mones_like(x, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### define objective function\n",
    "\n",
    "def objective(epochs=1):\n",
    "\n",
    "    # Optimize hyperparameters\n",
    "    # dropout = trial.suggest_float(\"dropout\", 0.01, 0.5)\n",
    "    # initial_learning_rate = trial.suggest_float(\"lr\", 1e-4, 1e-3) \n",
    "    \n",
    "    dropout = 0.1\n",
    "    initial_learning_rate = 5e-4\n",
    "    batch_size=128\n",
    "    \n",
    "    # Create inference net\n",
    "    \n",
    "    inference_net = bf.networks.CouplingFlow(coupling_kwargs=dict(subnet_kwargs=dict(dropout=dropout)))\n",
    "\n",
    "    # inference_net = bf.networks.FlowMatching(subnet_kwargs=dict(dropout=0.1))\n",
    "\n",
    "    summary_net = bf.networks.SetTransformer(summary_dim=32, num_seeds=2, dropout=0.1)\n",
    "    \n",
    "    \n",
    "    workflow = bf.BasicWorkflow(\n",
    "        simulator=simulator,\n",
    "        adapter=adapter,\n",
    "        initial_learning_rate=initial_learning_rate,\n",
    "        inference_network=inference_net,\n",
    "        summary_network=summary_net,\n",
    "        # checkpoint_filepath='../checkpoints',\n",
    "        # checkpoint_name= \"simons_crazy_net3\",\n",
    "        inference_variables=[\"A\", \"tau\", \"mu_c\", \"t0\", \"b\"])\n",
    "    \n",
    "    history = workflow.fit_offline(train_data, epochs=epochs, batch_size=batch_size, validation_data=val_data)\n",
    "    \n",
    "    metrics_table=workflow.compute_default_diagnostics(test_data=val_data)\n",
    "\n",
    "    # compute weighted sum\n",
    "    weighted_sum=weighted_metric_sum(metrics_table)\n",
    "    \n",
    "    # loss=np.mean(history.history[\"val_loss\"][-5:])\n",
    "        \n",
    "    return weighted_sum\n",
    "\n",
    "objective_test=objective()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8598938856409903"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objective_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def objective(trial, epochs=50):\n",
    "\n",
    "    # Optimize hyperparameters\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.01, 0.3)\n",
    "    initial_learning_rate = trial.suggest_float(\"lr\", 1e-4, 1e-3) \n",
    "    num_seeds=trial.suggest_int(\"num_seeds\", 1, 4)\n",
    "    depth=trial.suggest_int(\"depth\", 5, 10)\n",
    "    \n",
    "    batch_size=128\n",
    "    \n",
    "    # Create inference net\n",
    "    \n",
    "    inference_net = bf.networks.CouplingFlow(coupling_kwargs=dict(subnet_kwargs=dict(dropout=dropout)), depth=depth)\n",
    "\n",
    "    # inference_net = bf.networks.FlowMatching(subnet_kwargs=dict(dropout=0.1))\n",
    "\n",
    "    summary_net = bf.networks.SetTransformer(summary_dim=32, num_seeds=num_seeds, dropout=dropout)\n",
    "    \n",
    "    \n",
    "    workflow = bf.BasicWorkflow(\n",
    "        simulator=simulator,\n",
    "        adapter=adapter,\n",
    "        initial_learning_rate=initial_learning_rate,\n",
    "        inference_network=inference_net,\n",
    "        summary_network=summary_net,\n",
    "        # checkpoint_filepath='../checkpoints',\n",
    "        # checkpoint_name= \"simons_crazy_net3\",\n",
    "        inference_variables=[\"A\", \"tau\", \"mu_c\", \"t0\", \"b\"])\n",
    "    \n",
    "    history = workflow.fit_offline(train_data, epochs=epochs, batch_size=batch_size, validation_data=val_data, verbose=0)\n",
    "    \n",
    "    metrics_table=workflow.compute_default_diagnostics(test_data=val_data)\n",
    "\n",
    "    # compute weighted sum\n",
    "    weighted_sum=weighted_metric_sum(metrics_table)\n",
    "    \n",
    "    # loss=np.mean(history.history[\"val_loss\"][-5:])\n",
    "        \n",
    "    return weighted_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-02 18:57:01,588] A new study created in memory with name: no-name-3df42576-5ff5-4a51-863e-3aa99e7bac20\n",
      "INFO:bayesflow:Fitting on dataset instance of OfflineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "\n",
    "study.optimize(objective, n_trials=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bayesflow:Fitting on dataset instance of OfflineDataset.\n",
      "INFO:bayesflow:Building on a test batch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 124ms/step - loss: 4.7947 - loss/inference_loss: 4.7947 - val_loss: 4.2981 - val_loss/inference_loss: 4.2981\n",
      "Epoch 2/10\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 124ms/step - loss: 2.9251 - loss/inference_loss: 2.9251 - val_loss: 3.0569 - val_loss/inference_loss: 3.0569\n",
      "Epoch 3/10\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 125ms/step - loss: 2.1602 - loss/inference_loss: 2.1602 - val_loss: 2.1239 - val_loss/inference_loss: 2.1239\n",
      "Epoch 4/10\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 126ms/step - loss: 1.6297 - loss/inference_loss: 1.6297 - val_loss: 1.6652 - val_loss/inference_loss: 1.6652\n",
      "Epoch 5/10\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 126ms/step - loss: 1.3089 - loss/inference_loss: 1.3089 - val_loss: 2.3758 - val_loss/inference_loss: 2.3758\n",
      "Epoch 6/10\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 126ms/step - loss: 1.0308 - loss/inference_loss: 1.0308 - val_loss: 2.7440 - val_loss/inference_loss: 2.7440\n",
      "Epoch 7/10\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 127ms/step - loss: 0.8608 - loss/inference_loss: 0.8608 - val_loss: 3.6498 - val_loss/inference_loss: 3.6498\n",
      "Epoch 8/10\n",
      "\u001b[1m100/391\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 126ms/step - loss: 0.7717 - loss/inference_loss: 0.7717"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation loss: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(objective()))\n",
      "Cell \u001b[0;32mIn[10], line 30\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(epochs)\u001b[0m\n\u001b[1;32m     17\u001b[0m summary_net \u001b[38;5;241m=\u001b[39m bf\u001b[38;5;241m.\u001b[39mnetworks\u001b[38;5;241m.\u001b[39mSetTransformer(summary_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, num_seeds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m     20\u001b[0m workflow \u001b[38;5;241m=\u001b[39m bf\u001b[38;5;241m.\u001b[39mBasicWorkflow(\n\u001b[1;32m     21\u001b[0m     simulator\u001b[38;5;241m=\u001b[39msimulator,\n\u001b[1;32m     22\u001b[0m     adapter\u001b[38;5;241m=\u001b[39madapter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# checkpoint_name= \"simons_crazy_net3\",\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     inference_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtau\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmu_c\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt0\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 30\u001b[0m history \u001b[38;5;241m=\u001b[39m workflow\u001b[38;5;241m.\u001b[39mfit_offline(train_data, epochs\u001b[38;5;241m=\u001b[39mepochs, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, validation_data\u001b[38;5;241m=\u001b[39mval_data)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m:])\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/bayesflow/workflows/basic_workflow.py:668\u001b[0m, in \u001b[0;36mBasicWorkflow.fit_offline\u001b[0;34m(self, data, epochs, batch_size, keep_optimizer, validation_data, **kwargs)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;124;03mTrain the approximator offline using a fixed dataset. This approach will be faster than online training,\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;124;03msince no computation time is spent in generating new data for each batch, but it assumes that simulations\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;124;03m    metric evolution over epochs.\u001b[39;00m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    666\u001b[0m dataset \u001b[38;5;241m=\u001b[39m OfflineDataset(data\u001b[38;5;241m=\u001b[39mdata, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, adapter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madapter)\n\u001b[0;32m--> 668\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[1;32m    669\u001b[0m     dataset, epochs, strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monline\u001b[39m\u001b[38;5;124m\"\u001b[39m, keep_optimizer\u001b[38;5;241m=\u001b[39mkeep_optimizer, validation_data\u001b[38;5;241m=\u001b[39mvalidation_data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    670\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/bayesflow/workflows/basic_workflow.py:858\u001b[0m, in \u001b[0;36mBasicWorkflow._fit\u001b[0;34m(self, dataset, epochs, strategy, keep_optimizer, validation_data, **kwargs)\u001b[0m\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapproximator\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, metrics\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetrics\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 858\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapproximator\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m    859\u001b[0m         dataset\u001b[38;5;241m=\u001b[39mdataset, epochs\u001b[38;5;241m=\u001b[39mepochs, validation_data\u001b[38;5;241m=\u001b[39mvalidation_data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    860\u001b[0m     )\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_training_finished()\n\u001b[1;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/bayesflow/approximators/continuous_approximator.py:202\u001b[0m, in \u001b[0;36mContinuousApproximator.fit\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    151\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;124;03m    Trains the approximator on the provided dataset or on-demand data generated from the given simulator.\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m    If `dataset` is not provided, a dataset is built from the `simulator`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03m        If both `dataset` and `simulator` are provided or neither is provided.\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, adapter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madapter)\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/bayesflow/approximators/approximator.py:136\u001b[0m, in \u001b[0;36mApproximator.fit\u001b[0;34m(self, dataset, simulator, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m     mock_data \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mtree\u001b[38;5;241m.\u001b[39mmap_structure(keras\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mconvert_to_tensor, mock_data)\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_from_data(mock_data)\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(dataset\u001b[38;5;241m=\u001b[39mdataset, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/bayesflow/approximators/backend_approximators/backend_approximator.py:22\u001b[0m, in \u001b[0;36mBackendApproximator.fit\u001b[0;34m(self, dataset, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, dataset: keras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mPyDataset, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(x\u001b[38;5;241m=\u001b[39mdataset, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfilter_kwargs(kwargs, \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit))\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/keras/src/backend/torch/trainer.py:257\u001b[0m, in \u001b[0;36mTorchTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, data \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;66;03m# Callbacks\u001b[39;00m\n\u001b[1;32m    255\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 257\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(data)\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Callbacks\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/keras/src/backend/torch/trainer.py:117\u001b[0m, in \u001b[0;36mTorchTrainer.make_train_function.<locals>.one_step_on_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs a single training step on a batch of data.\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m data \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_step(data)\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/bayesflow/approximators/backend_approximators/torch_approximator.py:20\u001b[0m, in \u001b[0;36mTorchApproximator.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[1;32m     19\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m filter_kwargs(data \u001b[38;5;241m|\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m}, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics)\n\u001b[0;32m---> 20\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# noinspection PyUnresolvedReferences\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/bayesflow/approximators/continuous_approximator.py:137\u001b[0m, in \u001b[0;36mContinuousApproximator.compute_metrics\u001b[0;34m(self, inference_variables, inference_conditions, summary_variables, sample_weight, stage)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Force a conversion to Tensor\u001b[39;00m\n\u001b[1;32m    136\u001b[0m inference_variables \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mtree\u001b[38;5;241m.\u001b[39mmap_structure(keras\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mconvert_to_tensor, inference_variables)\n\u001b[0;32m--> 137\u001b[0m inference_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference_network\u001b[38;5;241m.\u001b[39mcompute_metrics(\n\u001b[1;32m    138\u001b[0m     inference_variables, conditions\u001b[38;5;241m=\u001b[39minference_conditions, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, stage\u001b[38;5;241m=\u001b[39mstage\n\u001b[1;32m    139\u001b[0m )\n\u001b[1;32m    141\u001b[0m loss \u001b[38;5;241m=\u001b[39m inference_metrics\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m, keras\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mzeros(())) \u001b[38;5;241m+\u001b[39m summary_metrics\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m, keras\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mzeros(()))\n\u001b[1;32m    143\u001b[0m inference_metrics \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/inference_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m inference_metrics\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/bayesflow/networks/coupling_flow/coupling_flow.py:165\u001b[0m, in \u001b[0;36mCouplingFlow.compute_metrics\u001b[0;34m(self, x, conditions, sample_weight, stage)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28mprint\u001b[39m(sample_weight)\n\u001b[1;32m    163\u001b[0m base_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcompute_metrics(x, conditions\u001b[38;5;241m=\u001b[39mconditions, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, stage\u001b[38;5;241m=\u001b[39mstage)\n\u001b[0;32m--> 165\u001b[0m z, log_density \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(x, conditions\u001b[38;5;241m=\u001b[39mconditions, inverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, density\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggregate(\u001b[38;5;241m-\u001b[39mlog_density, sample_weight)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m base_metrics \u001b[38;5;241m|\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss}\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/keras/src/layers/layer.py:909\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    907\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 909\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    910\u001b[0m \u001b[38;5;66;03m# Change the layout for the layer output if needed.\u001b[39;00m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;66;03m# This is useful for relayout intermediate tensor in the model\u001b[39;00m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;66;03m# to achieve the optimal performance.\u001b[39;00m\n\u001b[1;32m    913\u001b[0m distribution \u001b[38;5;241m=\u001b[39m distribution_lib\u001b[38;5;241m.\u001b[39mdistribution()\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/keras/src/backend/torch/layer.py:41\u001b[0m, in \u001b[0;36mTorchLayer.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Operation\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/keras/src/ops/operation.py:52\u001b[0m, in \u001b[0;36mOperation.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m                 call_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall\n\u001b[1;32m     48\u001b[0m     call_fn \u001b[38;5;241m=\u001b[39m traceback_utils\u001b[38;5;241m.\u001b[39minject_argument_info_in_traceback(\n\u001b[1;32m     49\u001b[0m         call_fn,\n\u001b[1;32m     50\u001b[0m         object_name\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.call()\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     51\u001b[0m     )\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m call_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:156\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/bayesflow/networks/inference_network.py:29\u001b[0m, in \u001b[0;36mInferenceNetwork.call\u001b[0;34m(self, xz, conditions, inverse, density, training, **kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inverse:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inverse(xz, conditions\u001b[38;5;241m=\u001b[39mconditions, density\u001b[38;5;241m=\u001b[39mdensity, training\u001b[38;5;241m=\u001b[39mtraining, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(xz, conditions\u001b[38;5;241m=\u001b[39mconditions, density\u001b[38;5;241m=\u001b[39mdensity, training\u001b[38;5;241m=\u001b[39mtraining, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/bayesflow/networks/coupling_flow/coupling_flow.py:132\u001b[0m, in \u001b[0;36mCouplingFlow._forward\u001b[0;34m(self, x, conditions, density, training, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m log_det \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mzeros(keras\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mshape(x)[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvertible_layers:\n\u001b[0;32m--> 132\u001b[0m     z, det \u001b[38;5;241m=\u001b[39m layer(z, conditions\u001b[38;5;241m=\u001b[39mconditions, inverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, training\u001b[38;5;241m=\u001b[39mtraining, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    133\u001b[0m     log_det \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m det\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m density:\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/keras/src/layers/layer.py:785\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;129m@traceback_utils\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_traceback\n\u001b[1;32m    783\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_super_called()\n\u001b[0;32m--> 785\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;66;03m#####################################\u001b[39;00m\n\u001b[1;32m    788\u001b[0m     \u001b[38;5;66;03m# 1. Convert any array arguments to tensors of correct dtype.\u001b[39;00m\n\u001b[1;32m    789\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmaybe_convert\u001b[39m(x):\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/keras/src/layers/layer.py:1467\u001b[0m, in \u001b[0;36mLayer.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1465\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_tracker\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1466\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_tracker()\n\u001b[0;32m-> 1467\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tracker\u001b[38;5;241m.\u001b[39mtrack(value)\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(name, value)\n",
      "File \u001b[0;32m~/anaconda3/envs/bf/lib/python3.11/site-packages/keras/src/utils/tracking.py:70\u001b[0m, in \u001b[0;36mTracker.track\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock_violation_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexclusions \u001b[38;5;241m=\u001b[39m exclusions \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m---> 70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrack\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr):\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tracking_enabled():\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m attr\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trial = study.best_trial\n",
    "print(\"Validation loss: {}\".format(trial.value))\n",
    "print(\"Best hyperparameters: {}\".format(trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
